{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Convolutional Neural Network (GCN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a complete [Tensorflow](https://www.tensorflow.org/) implementation of a two-layer graph convolutional neural network (GCN) for link prediction and it follows the GCN formulation as presented in [Kipf et al., ICLR 2017](https://arxiv.org/pdf/1609.02907.pdf). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "import networkx as nx\n",
    "import tensorflow.compat.v1 as tf\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "import os\n",
    "os.environ['VISIBLE_CUDA_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\marysia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\marysia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (2.0.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\marysia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (1.15.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\marysia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\marysia\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (3.5.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<absl.flags._flagvalues.FlagHolder at 0x1fd2dfeed10>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set random seed\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "# Settings\n",
    "flags = tf.app.flags\n",
    "for name in list(flags.FLAGS):\n",
    "      delattr(flags.FLAGS,name)\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_float('learning_rate', 0.01, 'Initial learning rate.')\n",
    "flags.DEFINE_integer('epochs', 100, 'Number of epochs to train.')\n",
    "flags.DEFINE_integer('hidden1', 32, 'Number of units in hidden layer 1.')\n",
    "flags.DEFINE_integer('hidden2', 16, 'Number of units in hidden layer 2.')\n",
    "flags.DEFINE_float('dropout', 0.1, 'Dropout rate (1 - keep probability).')\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Various Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(file_name: str):\n",
    "    g = nx.read_edgelist(file_name)\n",
    "    adj = nx.adjacency_matrix(g)\n",
    "    return adj\n",
    "\n",
    "\n",
    "def weight_variable_glorot(input_dim, output_dim, name=\"\"):\n",
    "    init_range = np.sqrt(6.0 / (input_dim + output_dim))\n",
    "    initial = tf.random_uniform(\n",
    "        [input_dim, output_dim], minval=-init_range,\n",
    "        maxval=init_range, dtype=tf.float32)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "\n",
    "def dropout_sparse(x, keep_prob, num_nonzero_elems):\n",
    "    noise_shape = [num_nonzero_elems]\n",
    "    random_tensor = keep_prob\n",
    "    random_tensor += tf.random_uniform(noise_shape)\n",
    "    dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool)\n",
    "    pre_out = tf.sparse_retain(x, dropout_mask)\n",
    "    return pre_out * (1. / keep_prob)\n",
    "\n",
    "\n",
    "def sparse_to_tuple(sparse_mx):\n",
    "    if not sp.isspmatrix_coo(sparse_mx):\n",
    "        sparse_mx = sparse_mx.tocoo()\n",
    "    coords = np.vstack((sparse_mx.row, sparse_mx.col)).transpose()\n",
    "    values = sparse_mx.data\n",
    "    shape = sparse_mx.shape\n",
    "    return coords, values, shape\n",
    "\n",
    "\n",
    "def preprocess_graph(adj):\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    adj_ = adj + sp.eye(adj.shape[0])\n",
    "    rowsum = np.array(adj_.sum(1))\n",
    "    degree_mat_inv_sqrt = sp.diags(np.power(rowsum, -0.5).flatten())\n",
    "    adj_normalized = adj_.dot(degree_mat_inv_sqrt).transpose().dot(degree_mat_inv_sqrt).tocoo()\n",
    "    return sparse_to_tuple(adj_normalized)\n",
    "\n",
    "\n",
    "def construct_feed_dict(adj_normalized, adj, features, placeholders):\n",
    "    feed_dict = dict()\n",
    "    feed_dict.update({placeholders['features']: features})\n",
    "    feed_dict.update({placeholders['adj']: adj_normalized})\n",
    "    feed_dict.update({placeholders['adj_orig']: adj})\n",
    "    return feed_dict\n",
    "\n",
    "\n",
    "def mask_test_edges(adj):\n",
    "    # Function to build test set with 2% positive links\n",
    "    # Remove diagonal elements\n",
    "    adj = adj - sp.dia_matrix((adj.diagonal()[np.newaxis, :], [0]), shape=adj.shape)\n",
    "    adj.eliminate_zeros()\n",
    "\n",
    "    adj_triu = sp.triu(adj)\n",
    "    adj_tuple = sparse_to_tuple(adj_triu)\n",
    "    edges = adj_tuple[0]\n",
    "    edges_all = sparse_to_tuple(adj)[0]\n",
    "    num_test = int(np.floor(edges.shape[0] / 50.))\n",
    "    num_val = int(np.floor(edges.shape[0] / 50.))\n",
    "\n",
    "    all_edge_idx = list(range(edges.shape[0]))\n",
    "    np.random.shuffle(all_edge_idx)\n",
    "    val_edge_idx = all_edge_idx[:num_val]\n",
    "    test_edge_idx = all_edge_idx[num_val:(num_val + num_test)]\n",
    "    test_edges = edges[test_edge_idx]\n",
    "    val_edges = edges[val_edge_idx]\n",
    "    train_edges = np.delete(edges, np.hstack([test_edge_idx, val_edge_idx]), axis=0)\n",
    "\n",
    "    def ismember(a, b):\n",
    "        rows_close = np.all((a - b[:, None]) == 0, axis=-1)\n",
    "        return np.any(rows_close)\n",
    "\n",
    "    test_edges_false = []\n",
    "    while len(test_edges_false) < len(test_edges):\n",
    "        n_rnd = len(test_edges) - len(test_edges_false)\n",
    "        rnd = np.random.randint(0, adj.shape[0], size=2 * n_rnd)\n",
    "        idxs_i = rnd[:n_rnd]                                        \n",
    "        idxs_j = rnd[n_rnd:]\n",
    "        for i in range(n_rnd):\n",
    "            idx_i = idxs_i[i]\n",
    "            idx_j = idxs_j[i]\n",
    "            if idx_i == idx_j:\n",
    "                continue\n",
    "            if ismember([idx_i, idx_j], edges_all):\n",
    "                continue\n",
    "            if test_edges_false:\n",
    "                if ismember([idx_j, idx_i], np.array(test_edges_false)):\n",
    "                    continue\n",
    "                if ismember([idx_i, idx_j], np.array(test_edges_false)):\n",
    "                    continue\n",
    "            test_edges_false.append([idx_i, idx_j])\n",
    "\n",
    "    val_edges_false = []\n",
    "    while len(val_edges_false) < len(val_edges):\n",
    "        n_rnd = len(val_edges) - len(val_edges_false)\n",
    "        rnd = np.random.randint(0, adj.shape[0], size=2 * n_rnd)\n",
    "        idxs_i = rnd[:n_rnd]                                        \n",
    "        idxs_j = rnd[n_rnd:]\n",
    "        for i in range(n_rnd):\n",
    "            idx_i = idxs_i[i]\n",
    "            idx_j = idxs_j[i]\n",
    "            if idx_i == idx_j:\n",
    "                continue\n",
    "            if ismember([idx_i, idx_j], train_edges):\n",
    "                continue\n",
    "            if ismember([idx_j, idx_i], train_edges):\n",
    "                continue\n",
    "            if ismember([idx_i, idx_j], val_edges):\n",
    "                continue\n",
    "            if ismember([idx_j, idx_i], val_edges):\n",
    "                continue\n",
    "            if val_edges_false:\n",
    "                if ismember([idx_j, idx_i], np.array(val_edges_false)):\n",
    "                    continue\n",
    "                if ismember([idx_i, idx_j], np.array(val_edges_false)):\n",
    "                    continue\n",
    "            val_edges_false.append([idx_i, idx_j])\n",
    "\n",
    "    # Re-build adj matrix\n",
    "    data = np.ones(train_edges.shape[0])\n",
    "    adj_train = sp.csr_matrix((data, (train_edges[:, 0], train_edges[:, 1])), shape=adj.shape)\n",
    "    adj_train = adj_train + adj_train.T\n",
    "\n",
    "    return adj_train, train_edges, val_edges, val_edges_false, test_edges, test_edges_false\n",
    "\n",
    "\n",
    "def get_roc_score(edges_pos, edges_neg):\n",
    "    feed_dict.update({placeholders['dropout']: 0})\n",
    "    emb = sess.run(model.embeddings, feed_dict=feed_dict)\n",
    "\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    # Predict on test set of edges\n",
    "    adj_rec = np.dot(emb, emb.T)\n",
    "    preds = []\n",
    "    pos = []\n",
    "    for e in edges_pos:\n",
    "        preds.append(sigmoid(adj_rec[e[0], e[1]]))\n",
    "        pos.append(adj_orig[e[0], e[1]])\n",
    "\n",
    "    preds_neg = []\n",
    "    neg = []\n",
    "    for e in edges_neg:\n",
    "        preds_neg.append(sigmoid(adj_rec[e[0], e[1]]))\n",
    "        neg.append(adj_orig[e[0], e[1]])\n",
    "\n",
    "    preds_all = np.hstack([preds, preds_neg])\n",
    "    labels_all = np.hstack([np.ones(len(preds)), np.zeros(len(preds))])\n",
    "    roc_score = roc_auc_score(labels_all, preds_all)\n",
    "    ap_score = average_precision_score(labels_all, preds_all)\n",
    "\n",
    "    return roc_score, ap_score, labels_all, preds_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Convolutional Layers for our GCN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GraphConvolution():\n",
    "    \"\"\"Basic graph convolution layer for undirected graph without edge labels.\"\"\"\n",
    "    def __init__(self, input_dim, output_dim, adj, name, dropout=0., act=tf.nn.relu):\n",
    "        self.name = name\n",
    "        self.vars = {}\n",
    "        self.issparse = False\n",
    "        with tf.variable_scope(self.name + '_vars'):\n",
    "            self.vars['weights'] = weight_variable_glorot(input_dim, output_dim, name='weights')\n",
    "        self.dropout = dropout\n",
    "        self.adj = adj\n",
    "        self.act = act\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        with tf.name_scope(self.name):        \n",
    "            x = inputs\n",
    "            x = tf.nn.dropout(x, 1-self.dropout)\n",
    "            x = tf.matmul(x, self.vars['weights'])\n",
    "            x = tf.sparse_tensor_dense_matmul(self.adj, x)\n",
    "            outputs = self.act(x)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class GraphConvolutionSparse():\n",
    "    \"\"\"Graph convolution layer for sparse inputs.\"\"\"\n",
    "    def __init__(self, input_dim, output_dim, adj, features_nonzero, name, dropout=0., act=tf.nn.relu):\n",
    "        self.name = name\n",
    "        self.vars = {}\n",
    "        self.issparse = False\n",
    "        with tf.variable_scope(self.name + '_vars'):\n",
    "            self.vars['weights'] = weight_variable_glorot(input_dim, output_dim, name='weights')\n",
    "        self.dropout = dropout\n",
    "        self.adj = adj\n",
    "        self.act = act\n",
    "        self.issparse = True\n",
    "        self.features_nonzero = features_nonzero\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        with tf.name_scope(self.name):\n",
    "            x = inputs\n",
    "            x = dropout_sparse(x, 1-self.dropout, self.features_nonzero)\n",
    "            x = tf.sparse_tensor_dense_matmul(x, self.vars['weights'])\n",
    "            x = tf.sparse_tensor_dense_matmul(self.adj, x)\n",
    "            outputs = self.act(x)\n",
    "        return outputs\n",
    "    \n",
    "    \n",
    "class InnerProductDecoder():\n",
    "    \"\"\"Decoder model layer for link prediction.\"\"\"\n",
    "    def __init__(self, input_dim, name, dropout=0., act=tf.nn.sigmoid):\n",
    "        self.name = name\n",
    "        self.issparse = False\n",
    "        self.dropout = dropout\n",
    "        self.act = act\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        with tf.name_scope(self.name):\n",
    "            inputs = tf.nn.dropout(inputs, 1-self.dropout)\n",
    "            x = tf.transpose(inputs)\n",
    "            x = tf.matmul(inputs, x)\n",
    "            x = tf.reshape(x, [-1])\n",
    "            outputs = self.act(x)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify the Architecture of our GCN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GCNModel():\n",
    "    def __init__(self, placeholders, num_features, features_nonzero, name):\n",
    "        self.name = name\n",
    "        self.inputs = placeholders['features']\n",
    "        self.input_dim = num_features\n",
    "        self.features_nonzero = features_nonzero\n",
    "        self.adj = placeholders['adj']\n",
    "        self.dropout = placeholders['dropout']\n",
    "        with tf.variable_scope(self.name):\n",
    "            self.build()\n",
    "        \n",
    "    def build(self):\n",
    "        self.hidden1 = GraphConvolutionSparse(\n",
    "            name='gcn_sparse_layer',\n",
    "            input_dim=self.input_dim,\n",
    "            output_dim=FLAGS.hidden1,\n",
    "            adj=self.adj,\n",
    "            features_nonzero=self.features_nonzero,\n",
    "            act=tf.nn.relu,\n",
    "            dropout=self.dropout)(self.inputs)\n",
    "\n",
    "        self.embeddings = GraphConvolution(\n",
    "            name='gcn_dense_layer',\n",
    "            input_dim=FLAGS.hidden1,\n",
    "            output_dim=FLAGS.hidden2,\n",
    "            adj=self.adj,\n",
    "            act=lambda x: x,\n",
    "            dropout=self.dropout)(self.hidden1)\n",
    "\n",
    "        self.reconstructions = InnerProductDecoder(\n",
    "            name='gcn_decoder',\n",
    "            input_dim=FLAGS.hidden2, \n",
    "            act=lambda x: x)(self.embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify the GCN Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Optimizer():\n",
    "    def __init__(self, preds, labels, num_nodes, num_edges):\n",
    "        pos_weight = float(num_nodes**2 - num_edges) / num_edges\n",
    "        norm = num_nodes**2 / float((num_nodes**2 - num_edges) * 2)\n",
    "        \n",
    "        preds_sub = preds\n",
    "        labels_sub = labels\n",
    "\n",
    "        self.cost = norm * tf.reduce_mean(\n",
    "            tf.nn.weighted_cross_entropy_with_logits(\n",
    "                logits=preds_sub, targets=labels_sub, pos_weight=pos_weight))\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=FLAGS.learning_rate)  # Adam Optimizer\n",
    "\n",
    "        self.opt_op = self.optimizer.minimize(self.cost)\n",
    "        self.grads_vars = self.optimizer.compute_gradients(self.cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the GCN Model and Evaluate its Accuracy on a Test Set of Protein-Protein Interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a training set of protein-protein interactions in yeast *S. cerevisiae*, our goal is to take these interactions and train a GCN model that can predict new protein-protein interactions. That is, we would like to predict new edges in the yeast protein interaction network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adj = load_data('combined_network.txt')\n",
    "num_nodes = adj.shape[0]\n",
    "num_edges = adj.sum()\n",
    "# Featureless\n",
    "features = sparse_to_tuple(sp.identity(num_nodes))\n",
    "num_features = features[2][1]\n",
    "features_nonzero = features[1].shape[0]\n",
    "\n",
    "# Store original adjacency matrix (without diagonal entries) for later\n",
    "adj_orig = adj - sp.dia_matrix((adj.diagonal()[np.newaxis, :], [0]), shape=adj.shape)\n",
    "adj_orig.eliminate_zeros()\n",
    "\n",
    "adj_train, train_edges, val_edges, val_edges_false, test_edges, test_edges_false = mask_test_edges(adj)\n",
    "adj = adj_train\n",
    "\n",
    "adj_norm = preprocess_graph(adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Marysia\\AppData\\Local\\Temp\\ipykernel_6372\\219024162.py:2: The name tf.disable_eager_execution is deprecated. Please use tf.compat.v1.disable_eager_execution instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Marysia\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1260: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From c:\\Users\\Marysia\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1260: calling weighted_cross_entropy_with_logits (from tensorflow.python.ops.nn_impl) with targets is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "targets is deprecated, use labels instead\n",
      "WARNING:tensorflow:From C:\\Users\\Marysia\\AppData\\Local\\Temp\\ipykernel_6372\\3543160075.py:12: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define placeholders\n",
    "tf.compat.v1.disable_eager_execution() #added because tf placeholder doesnt work without it\n",
    "placeholders = {\n",
    "    'features': tf.sparse_placeholder(tf.float32),\n",
    "    'adj': tf.sparse_placeholder(tf.float32),\n",
    "    'adj_orig': tf.sparse_placeholder(tf.float32),\n",
    "    'dropout': tf.placeholder_with_default(0., shape=())\n",
    "}\n",
    "\n",
    "# Create model\n",
    "model = GCNModel(placeholders, num_features, features_nonzero, name='yeast_gcn')\n",
    "\n",
    "# Create optimizer\n",
    "with tf.name_scope('optimizer'):\n",
    "    opt = Optimizer(\n",
    "        preds=model.reconstructions,\n",
    "        labels=tf.reshape(tf.sparse_tensor_to_dense(placeholders['adj_orig'], validate_indices=False), [-1]),\n",
    "        num_nodes=num_nodes,\n",
    "        num_edges=num_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 train_loss= 0.69242 val_roc= 0.88082 val_ap= 0.87640 time= 1.04266\n",
      "Epoch: 0002 train_loss= 0.69210 val_roc= 0.90528 val_ap= 0.90246 time= 0.12789\n",
      "Epoch: 0003 train_loss= 0.69087 val_roc= 0.90175 val_ap= 0.90398 time= 0.12013\n",
      "Epoch: 0004 train_loss= 0.68845 val_roc= 0.89742 val_ap= 0.90262 time= 0.11829\n",
      "Epoch: 0005 train_loss= 0.68328 val_roc= 0.89220 val_ap= 0.90027 time= 0.11444\n",
      "Epoch: 0006 train_loss= 0.67642 val_roc= 0.88743 val_ap= 0.89793 time= 0.12749\n",
      "Epoch: 0007 train_loss= 0.66611 val_roc= 0.88472 val_ap= 0.89672 time= 0.11335\n",
      "Epoch: 0008 train_loss= 0.65008 val_roc= 0.88249 val_ap= 0.89561 time= 0.12790\n",
      "Epoch: 0009 train_loss= 0.63179 val_roc= 0.88049 val_ap= 0.89447 time= 0.13191\n",
      "Epoch: 0010 train_loss= 0.61489 val_roc= 0.87942 val_ap= 0.89375 time= 0.10961\n",
      "Epoch: 0011 train_loss= 0.59354 val_roc= 0.87801 val_ap= 0.89279 time= 0.11964\n",
      "Epoch: 0012 train_loss= 0.58400 val_roc= 0.87673 val_ap= 0.89185 time= 0.11557\n",
      "Epoch: 0013 train_loss= 0.58510 val_roc= 0.87615 val_ap= 0.89152 time= 0.12835\n",
      "Epoch: 0014 train_loss= 0.58325 val_roc= 0.87498 val_ap= 0.89008 time= 0.13793\n",
      "Epoch: 0015 train_loss= 0.58156 val_roc= 0.87509 val_ap= 0.88989 time= 0.11880\n",
      "Epoch: 0016 train_loss= 0.57038 val_roc= 0.87569 val_ap= 0.88977 time= 0.11006\n",
      "Epoch: 0017 train_loss= 0.56157 val_roc= 0.87521 val_ap= 0.88848 time= 0.11545\n",
      "Epoch: 0018 train_loss= 0.55545 val_roc= 0.87459 val_ap= 0.88848 time= 0.11655\n",
      "Epoch: 0019 train_loss= 0.54515 val_roc= 0.87279 val_ap= 0.88808 time= 0.11198\n",
      "Epoch: 0020 train_loss= 0.54016 val_roc= 0.87008 val_ap= 0.88731 time= 0.12346\n",
      "Epoch: 0021 train_loss= 0.53781 val_roc= 0.86633 val_ap= 0.88449 time= 0.10815\n",
      "Epoch: 0022 train_loss= 0.53591 val_roc= 0.86268 val_ap= 0.88087 time= 0.11213\n",
      "Epoch: 0023 train_loss= 0.53667 val_roc= 0.85874 val_ap= 0.87636 time= 0.11472\n",
      "Epoch: 0024 train_loss= 0.53413 val_roc= 0.85507 val_ap= 0.87295 time= 0.11964\n",
      "Epoch: 0025 train_loss= 0.53159 val_roc= 0.85323 val_ap= 0.87133 time= 0.11045\n",
      "Epoch: 0026 train_loss= 0.52789 val_roc= 0.85215 val_ap= 0.87061 time= 0.13091\n",
      "Epoch: 0027 train_loss= 0.52714 val_roc= 0.85192 val_ap= 0.87054 time= 0.12200\n",
      "Epoch: 0028 train_loss= 0.52690 val_roc= 0.85221 val_ap= 0.87118 time= 0.12929\n",
      "Epoch: 0029 train_loss= 0.52565 val_roc= 0.85323 val_ap= 0.87230 time= 0.13230\n",
      "Epoch: 0030 train_loss= 0.52634 val_roc= 0.85449 val_ap= 0.87388 time= 0.13143\n",
      "Epoch: 0031 train_loss= 0.52646 val_roc= 0.85528 val_ap= 0.87508 time= 0.10719\n",
      "Epoch: 0032 train_loss= 0.52376 val_roc= 0.85605 val_ap= 0.87604 time= 0.12934\n",
      "Epoch: 0033 train_loss= 0.52484 val_roc= 0.85751 val_ap= 0.87722 time= 0.12094\n",
      "Epoch: 0034 train_loss= 0.52133 val_roc= 0.85870 val_ap= 0.87844 time= 0.11026\n",
      "Epoch: 0035 train_loss= 0.52082 val_roc= 0.85955 val_ap= 0.88002 time= 0.10100\n",
      "Epoch: 0036 train_loss= 0.51704 val_roc= 0.86055 val_ap= 0.88130 time= 0.11338\n",
      "Epoch: 0037 train_loss= 0.51438 val_roc= 0.86168 val_ap= 0.88282 time= 0.11184\n",
      "Epoch: 0038 train_loss= 0.51238 val_roc= 0.86372 val_ap= 0.88536 time= 0.12937\n",
      "Epoch: 0039 train_loss= 0.51296 val_roc= 0.86631 val_ap= 0.88802 time= 0.11299\n",
      "Epoch: 0040 train_loss= 0.50957 val_roc= 0.86910 val_ap= 0.89105 time= 0.11177\n",
      "Epoch: 0041 train_loss= 0.50722 val_roc= 0.87232 val_ap= 0.89422 time= 0.10670\n",
      "Epoch: 0042 train_loss= 0.50799 val_roc= 0.87481 val_ap= 0.89676 time= 0.10907\n",
      "Epoch: 0043 train_loss= 0.50472 val_roc= 0.87732 val_ap= 0.89872 time= 0.10504\n",
      "Epoch: 0044 train_loss= 0.50136 val_roc= 0.87961 val_ap= 0.90046 time= 0.11439\n",
      "Epoch: 0045 train_loss= 0.49735 val_roc= 0.88236 val_ap= 0.90266 time= 0.10174\n",
      "Epoch: 0046 train_loss= 0.49552 val_roc= 0.88541 val_ap= 0.90514 time= 0.11615\n",
      "Epoch: 0047 train_loss= 0.49240 val_roc= 0.88841 val_ap= 0.90738 time= 0.10424\n",
      "Epoch: 0048 train_loss= 0.49070 val_roc= 0.89114 val_ap= 0.90919 time= 0.10970\n",
      "Epoch: 0049 train_loss= 0.48687 val_roc= 0.89323 val_ap= 0.91100 time= 0.11133\n",
      "Epoch: 0050 train_loss= 0.48263 val_roc= 0.89558 val_ap= 0.91293 time= 0.10650\n",
      "Epoch: 0051 train_loss= 0.47964 val_roc= 0.89765 val_ap= 0.91435 time= 0.10802\n",
      "Epoch: 0052 train_loss= 0.47641 val_roc= 0.89942 val_ap= 0.91572 time= 0.11411\n",
      "Epoch: 0053 train_loss= 0.47453 val_roc= 0.90006 val_ap= 0.91665 time= 0.10589\n",
      "Epoch: 0054 train_loss= 0.47285 val_roc= 0.90263 val_ap= 0.91917 time= 0.11210\n",
      "Epoch: 0055 train_loss= 0.46836 val_roc= 0.90474 val_ap= 0.92095 time= 0.10127\n",
      "Epoch: 0056 train_loss= 0.46664 val_roc= 0.90590 val_ap= 0.92221 time= 0.11881\n",
      "Epoch: 0057 train_loss= 0.46588 val_roc= 0.90688 val_ap= 0.92304 time= 0.13377\n",
      "Epoch: 0058 train_loss= 0.46460 val_roc= 0.90801 val_ap= 0.92450 time= 0.12829\n",
      "Epoch: 0059 train_loss= 0.46428 val_roc= 0.90876 val_ap= 0.92554 time= 0.11533\n",
      "Epoch: 0060 train_loss= 0.46279 val_roc= 0.91001 val_ap= 0.92723 time= 0.10462\n",
      "Epoch: 0061 train_loss= 0.46213 val_roc= 0.91080 val_ap= 0.92823 time= 0.10448\n",
      "Epoch: 0062 train_loss= 0.46049 val_roc= 0.91120 val_ap= 0.92853 time= 0.11523\n",
      "Epoch: 0063 train_loss= 0.46059 val_roc= 0.91189 val_ap= 0.92896 time= 0.10903\n",
      "Epoch: 0064 train_loss= 0.45969 val_roc= 0.91239 val_ap= 0.92924 time= 0.10830\n",
      "Epoch: 0065 train_loss= 0.46100 val_roc= 0.91280 val_ap= 0.92952 time= 0.10269\n",
      "Epoch: 0066 train_loss= 0.45961 val_roc= 0.91320 val_ap= 0.92952 time= 0.11614\n",
      "Epoch: 0067 train_loss= 0.45809 val_roc= 0.91414 val_ap= 0.92995 time= 0.11008\n",
      "Epoch: 0068 train_loss= 0.45724 val_roc= 0.91501 val_ap= 0.93038 time= 0.10509\n",
      "Epoch: 0069 train_loss= 0.45683 val_roc= 0.91576 val_ap= 0.93081 time= 0.11701\n",
      "Epoch: 0070 train_loss= 0.45619 val_roc= 0.91668 val_ap= 0.93114 time= 0.11586\n",
      "Epoch: 0071 train_loss= 0.45552 val_roc= 0.91747 val_ap= 0.93124 time= 0.11819\n",
      "Epoch: 0072 train_loss= 0.45332 val_roc= 0.91812 val_ap= 0.93151 time= 0.12691\n",
      "Epoch: 0073 train_loss= 0.45431 val_roc= 0.91906 val_ap= 0.93174 time= 0.11984\n",
      "Epoch: 0074 train_loss= 0.45237 val_roc= 0.91983 val_ap= 0.93183 time= 0.13198\n",
      "Epoch: 0075 train_loss= 0.45248 val_roc= 0.92069 val_ap= 0.93195 time= 0.11532\n",
      "Epoch: 0076 train_loss= 0.45243 val_roc= 0.92183 val_ap= 0.93218 time= 0.13560\n",
      "Epoch: 0077 train_loss= 0.45189 val_roc= 0.92302 val_ap= 0.93274 time= 0.11487\n",
      "Epoch: 0078 train_loss= 0.45055 val_roc= 0.92406 val_ap= 0.93316 time= 0.12304\n",
      "Epoch: 0079 train_loss= 0.45098 val_roc= 0.92483 val_ap= 0.93362 time= 0.10908\n",
      "Epoch: 0080 train_loss= 0.45062 val_roc= 0.92544 val_ap= 0.93390 time= 0.13517\n",
      "Epoch: 0081 train_loss= 0.44968 val_roc= 0.92606 val_ap= 0.93434 time= 0.13016\n",
      "Epoch: 0082 train_loss= 0.45003 val_roc= 0.92642 val_ap= 0.93449 time= 0.13221\n",
      "Epoch: 0083 train_loss= 0.44973 val_roc= 0.92709 val_ap= 0.93521 time= 0.11691\n",
      "Epoch: 0084 train_loss= 0.44904 val_roc= 0.92738 val_ap= 0.93553 time= 0.12960\n",
      "Epoch: 0085 train_loss= 0.44864 val_roc= 0.92788 val_ap= 0.93607 time= 0.10899\n",
      "Epoch: 0086 train_loss= 0.44893 val_roc= 0.92775 val_ap= 0.93599 time= 0.11029\n",
      "Epoch: 0087 train_loss= 0.44853 val_roc= 0.92798 val_ap= 0.93620 time= 0.10959\n",
      "Epoch: 0088 train_loss= 0.44820 val_roc= 0.92813 val_ap= 0.93645 time= 0.11957\n",
      "Epoch: 0089 train_loss= 0.44834 val_roc= 0.92836 val_ap= 0.93670 time= 0.14520\n",
      "Epoch: 0090 train_loss= 0.44863 val_roc= 0.92823 val_ap= 0.93655 time= 0.13549\n",
      "Epoch: 0091 train_loss= 0.44740 val_roc= 0.92802 val_ap= 0.93645 time= 0.12920\n",
      "Epoch: 0092 train_loss= 0.44716 val_roc= 0.92805 val_ap= 0.93646 time= 0.13027\n",
      "Epoch: 0093 train_loss= 0.44647 val_roc= 0.92819 val_ap= 0.93657 time= 0.12426\n",
      "Epoch: 0094 train_loss= 0.44723 val_roc= 0.92836 val_ap= 0.93660 time= 0.12390\n",
      "Epoch: 0095 train_loss= 0.44684 val_roc= 0.92865 val_ap= 0.93700 time= 0.11761\n",
      "Epoch: 0096 train_loss= 0.44596 val_roc= 0.92855 val_ap= 0.93653 time= 0.15356\n",
      "Epoch: 0097 train_loss= 0.44579 val_roc= 0.92863 val_ap= 0.93659 time= 0.42810\n",
      "Epoch: 0098 train_loss= 0.44532 val_roc= 0.92863 val_ap= 0.93658 time= 0.55038\n",
      "Epoch: 0099 train_loss= 0.44628 val_roc= 0.92850 val_ap= 0.93652 time= 0.16856\n",
      "Epoch: 0100 train_loss= 0.44619 val_roc= 0.92815 val_ap= 0.93652 time= 0.12899\n",
      "Optimization Finished!\n",
      "Test ROC score: 0.95601\n",
      "Test AP score: 0.95779\n"
     ]
    }
   ],
   "source": [
    "# Initialize session\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "adj_label = adj_train + sp.eye(adj_train.shape[0])\n",
    "adj_label = sparse_to_tuple(adj_label)\n",
    "epoch = 0\n",
    "temp_train_loss = 1.0\n",
    "# Train model\n",
    "while (epoch < FLAGS.epochs) and (temp_train_loss > 0.001):\n",
    "    t = time.time()\n",
    "    # Construct feed dictionary\n",
    "    feed_dict = construct_feed_dict(adj_norm, adj_label, features, placeholders)\n",
    "    feed_dict.update({placeholders['dropout']: FLAGS.dropout})\n",
    "    # One update of parameter matrices\n",
    "    _, avg_cost = sess.run([opt.opt_op, opt.cost], feed_dict=feed_dict)\n",
    "    # Performance on validation set\n",
    "    roc_curr, ap_curr, labels_all, pred_all= get_roc_score(val_edges, val_edges_false)\n",
    "\n",
    "    print(\"Epoch:\", '%04d' % (epoch + 1), \n",
    "          \"train_loss=\", \"{:.5f}\".format(avg_cost),\n",
    "          \"val_roc=\", \"{:.5f}\".format(roc_curr),\n",
    "          \"val_ap=\", \"{:.5f}\".format(ap_curr),\n",
    "          \"time=\", \"{:.5f}\".format(time.time() - t))\n",
    "    epoch = epoch + 1\n",
    "    temp_train_loss = avg_cost\n",
    "\n",
    "print('Optimization Finished!')\n",
    "\n",
    "roc_score, ap_score, labels_all, pred_all= get_roc_score(test_edges, test_edges_false)\n",
    "print('Test ROC score: {:.5f}'.format(roc_score))\n",
    "print('Test AP score: {:.5f}'.format(ap_score))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'models//combined_trained20'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "saver.save(sess, 'models//combined_trained20')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"results//preds100.txt\", pred_all, fmt=\"%.4f\")\n",
    "np.savetxt(\"results//edges100.txt\", test_edges)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
